{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e980dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import datasets, transforms, utils\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f058cd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba92e167",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "71859634",
   "metadata": {
    "id": "G8vdR9BDFP0C"
   },
   "outputs": [],
   "source": [
    "def conv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    layers.append(conv_layer)\n",
    "    \n",
    "    if batch_norm:\n",
    "        bn = nn.BatchNorm2d(out_channels)\n",
    "        layers.append(bn)\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c71924d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, y_size, conv_dim=128):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         self.conv_dim = conv_dim\n",
    "        \n",
    "#         # Convolutional layers\n",
    "#         self.conv1 = nn.Conv2d(3, conv_dim, 4, 2, 1)\n",
    "#         self.conv2 = nn.Conv2d(conv_dim, conv_dim*2, 4, 2, 1)\n",
    "#         self.conv3 = nn.Conv2d(conv_dim*2, conv_dim*4, 4, 2, 1)\n",
    "#         self.conv4 = nn.Conv2d(conv_dim*4, conv_dim*8, 4, 2, 1)\n",
    "#         self.conv5 = nn.Conv2d(conv_dim*8 + y_size, 1, 4, 1, 0)\n",
    "        \n",
    "#     def forward(self, x, y):\n",
    "#         x = nn.LeakyReLU(0.2)(self.conv1(x))\n",
    "#         x = nn.LeakyReLU(0.2)(self.conv2(x))\n",
    "#         x = nn.LeakyReLU(0.2)(self.conv3(x))\n",
    "#         x = nn.LeakyReLU(0.2)(self.conv4(x))\n",
    "        \n",
    "#         # Concatenate the condition y to the features\n",
    "#         y = y.view(-1, y.size()[-1], 1, 1)\n",
    "#         y = y.expand(-1, -1, x.size(2), x.size(3))\n",
    "#         x = torch.cat([x, y], dim=1)\n",
    "        \n",
    "#         x = self.conv5(x)\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e38f826",
   "metadata": {
    "id": "3Gl3qi7eIgxG"
   },
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, y_size, conv_dim=64):\n",
    "\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.conv_dim = conv_dim\n",
    "        self.y_size = y_size\n",
    "        self.conv1 = conv(3, conv_dim, 4, batch_norm=False)\n",
    "        self.conv2 = conv(conv_dim+y_size, conv_dim * 2, 4)\n",
    "        self.conv3 = conv(conv_dim*2, conv_dim*4, 4)\n",
    "        self.conv4 = conv(conv_dim*4, conv_dim*8, 4)\n",
    "        self.conv5 = conv(conv_dim*8, 1, 4, 1, 0, batch_norm=False)\n",
    "            \n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x = F.relu(self.conv1(x))\n",
    "        y = y.view(-1,y.size()[-1],1,1)\n",
    "        y = y.expand(-1,-1,x.size()[-2], x.size()[-1])\n",
    "        x = torch.cat([x, y], 1)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "550e1c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels, out_channels, kernel_size=4, stride=2, padding=1, batch_norm=True):\n",
    "    \n",
    "    layers = []\n",
    "    t_conv = nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False)\n",
    "    layers.append(t_conv)\n",
    "    \n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "        \n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69446784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, z_size, y_size, conv_dim=128):\n",
    "#         super(Generator, self).__init__()\n",
    "        \n",
    "#         self.conv_dim = conv_dim\n",
    "        \n",
    "#         # Transposed convolutional layers\n",
    "#         self.t_conv1 = nn.ConvTranspose2d(z_size + y_size, conv_dim*8, 4, 1, 0)\n",
    "#         self.t_conv2 = nn.ConvTranspose2d(conv_dim*8, conv_dim*4, 4, 2, 1)\n",
    "#         self.t_conv3 = nn.ConvTranspose2d(conv_dim*4, conv_dim*2, 4, 2, 1)\n",
    "#         self.t_conv4 = nn.ConvTranspose2d(conv_dim*2, conv_dim, 4, 2, 1)\n",
    "#         self.t_conv5 = nn.ConvTranspose2d(conv_dim, 3, 4, 2, 1)\n",
    "        \n",
    "#     def forward(self, z, y):\n",
    "#         x = torch.cat([z, y], dim=1)\n",
    "#         x = x.view(-1, x.size()[-1], 1, 1)\n",
    "        \n",
    "#         x = nn.ReLU()(self.t_conv1(x))\n",
    "#         x = nn.ReLU()(self.t_conv2(x))\n",
    "#         x = nn.ReLU()(self.t_conv3(x))\n",
    "#         x = nn.ReLU()(self.t_conv4(x))\n",
    "#         x = torch.tanh(self.t_conv5(x))\n",
    "        \n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64817dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    \n",
    "    def __init__(self, z_size, y_size, conv_dim=64):\n",
    "\n",
    "        super(Generator, self).__init__()\n",
    "        \n",
    "        self.conv_dim = conv_dim\n",
    "        \n",
    "        self.t_conv1 = deconv(z_size+y_size, conv_dim*8, 4, 1, 0)\n",
    "        self.t_conv2 = deconv(conv_dim*8, conv_dim*4, 4)\n",
    "        self.t_conv3 = deconv(conv_dim*4, conv_dim*2, 4)\n",
    "        self.t_conv4 = deconv(conv_dim*2, conv_dim, 4)\n",
    "        self.t_conv5 = deconv(conv_dim, 3, 4, batch_norm=False)\n",
    "        \n",
    "    def forward(self, z, y):\n",
    "\n",
    "        x = torch.cat([z, y], dim=1)\n",
    "        x = x.view(-1, x.size()[-1], 1, 1)\n",
    "        x = F.relu(self.t_conv1(x))\n",
    "        x = F.relu(self.t_conv2(x))\n",
    "        x = F.relu(self.t_conv3(x))\n",
    "        x = F.relu(self.t_conv4(x))\n",
    "        x = self.t_conv5(x)\n",
    "        x = torch.tanh(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f4ae33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Generator\n",
    "# class Generator(nn.Module):\n",
    "#     def __init__(self, z_dim, y_dim, image_size, num_channels):\n",
    "#         super(Generator, self).__init__()\n",
    "        \n",
    "#         self.z_dim = z_dim\n",
    "#         self.y_dim = y_dim\n",
    "#         self.image_size = image_size\n",
    "#         self.num_channels = num_channels\n",
    "        \n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(z_dim + y_dim, 256 * 8 * 8),\n",
    "#             nn.ReLU()\n",
    "#         )\n",
    "        \n",
    "#         self.deconv_layers = nn.Sequential(\n",
    "#             nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "#             nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.ReLU(),\n",
    "            \n",
    "#             nn.ConvTranspose2d(64, num_channels, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.Tanh()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, z, y):\n",
    "#         zy = torch.cat([z, y], dim=1)\n",
    "#         x = self.fc(zy)\n",
    "#         x = x.view(x.size(0), 256, 8, 8)\n",
    "#         x = self.deconv_layers(x)\n",
    "#         return x\n",
    "\n",
    "# # Discriminator\n",
    "# class Discriminator(nn.Module):\n",
    "#     def __init__(self, image_size, y_dim, num_channels):\n",
    "#         super(Discriminator, self).__init__()\n",
    "        \n",
    "#         self.image_size = image_size\n",
    "#         self.y_dim = y_dim\n",
    "#         self.num_channels = num_channels\n",
    "        \n",
    "#         self.conv_layers = nn.Sequential(\n",
    "#             nn.Conv2d(num_channels + y_dim, 64, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "            \n",
    "#             nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(0.2),\n",
    "            \n",
    "#             nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(0.2)\n",
    "#         )\n",
    "        \n",
    "#         self.fc = nn.Sequential(\n",
    "#             nn.Linear(256 * 8 * 8, 1),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         y = y.view(-1, self.y_dim, 1, 1)\n",
    "#         y = y.expand(-1, -1, self.image_size, self.image_size)\n",
    "#         x = torch.cat([x, y], dim=1)\n",
    "#         x = self.conv_layers(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = self.fc(x)\n",
    "#         return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7873c79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discriminator(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(134, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(512, 1024, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(1024, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "Generator(\n",
      "  (t_conv1): Sequential(\n",
      "    (0): ConvTranspose2d(106, 1024, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv2): Sequential(\n",
      "    (0): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv3): Sequential(\n",
      "    (0): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv4): Sequential(\n",
      "    (0): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (t_conv5): Sequential(\n",
      "    (0): ConvTranspose2d(128, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "conv_dim = 128\n",
    "z_size = 100\n",
    "y_size = 6 \n",
    "\n",
    "D = Discriminator(y_size, conv_dim)\n",
    "G = Generator(z_size, y_size, conv_dim)\n",
    "\n",
    "print(D)\n",
    "print()\n",
    "print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb052e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialize the generator and discriminator\n",
    "# z_dim = 100\n",
    "# y_dim = 6\n",
    "# image_size = 64\n",
    "# num_channels = 3\n",
    "\n",
    "# G = Generator(z_dim, y_dim, image_size, num_channels)\n",
    "# D = Discriminator(image_size, y_dim, num_channels)\n",
    "\n",
    "# print(D)\n",
    "# print()\n",
    "# print(G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e06675bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [18, 29, 39, 49, 59]\n",
    "root_dir = \"D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54ace236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.load_state_dict(torch.load(os.path.join(root_dir,'GAN_mod_1_G.pth'), map_location=torch.device(device)))\n",
    "D.load_state_dict(torch.load(os.path.join(root_dir,'GAN_mod_1_D.pth'), map_location=torch.device(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d1e766",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ad75550",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\n"
     ]
    }
   ],
   "source": [
    "!cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d96fbd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done created\n"
     ]
    }
   ],
   "source": [
    "# Move your model to the specified device\n",
    "G = G.to(device)\n",
    "\n",
    "num_samples = 16\n",
    "z = torch.randn(num_samples, z_size, device=device)\n",
    "ages = torch.zeros(num_samples, len(bins) + 1, device=device).scatter_(\n",
    "    1, torch.tensor(np.random.randint(len(bins), size=num_samples).reshape(-1, 1), dtype=torch.long, device=device), 1\n",
    ")\n",
    "\n",
    "fake_images = G(z, ages).detach().cpu()\n",
    "\n",
    "os.makedirs(os.path.join(root_dir, 'samples'), exist_ok=True)\n",
    "print(\"done created\")\n",
    "\n",
    "for i in range(num_samples):\n",
    "    age_label = np.argmax(ages[i].cpu().numpy())\n",
    "    filename = os.path.join(root_dir, 'samples', 'sample_{}_age_{}.png'.format(i, age_label))\n",
    "    save_image(fake_images[i], filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "aeb33428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_0_age_3.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_1_age_0.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_2_age_0.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_3_age_0.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_4_age_2.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_5_age_1.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_6_age_0.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_7_age_3.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_8_age_1.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_9_age_1.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_10_age_2.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_11_age_0.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_12_age_2.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_13_age_3.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_14_age_1.png\n",
      "Saved sample image: D:\\study\\Project\\TARP project\\MissingChildIdentification-main\\MissingChildIdentification-main\\content\\Age-cGAN\\samples\\sample_15_age_3.png\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_samples):\n",
    "    age_label = np.argmax(ages[i].cpu().numpy())\n",
    "    filename = os.path.abspath(os.path.join(root_dir, 'samples', 'sample_{}_age_{}.png'.format(i, age_label)))\n",
    "    save_image(fake_images[i], filename)\n",
    "    print(\"Saved sample image:\", filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ceaff437",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set the model to evaluation mode\n",
    "# G.eval()\n",
    "\n",
    "# # Evaluate the model on the test set\n",
    "# test_loss = 0\n",
    "# mae = 0\n",
    "# num_batches = 0\n",
    "# with torch.no_grad():\n",
    "#     for batch in test_loader:\n",
    "#         # Get the batch size\n",
    "#         batch_size = batch['image'].size(0)\n",
    "        \n",
    "#         # Scale the images and one-hot encode the ages\n",
    "#         real_images = scale(batch['image'])\n",
    "#         ages = one_hot(batch['age'], bins)\n",
    "        \n",
    "#         # Move the data to the device\n",
    "#         real_images = real_images.to(device)\n",
    "#         ages = ages.to(device)\n",
    "        \n",
    "#         # Generate fake images and calculate the loss\n",
    "#         z = np.random.uniform(-1, 1, size=(batch_size, z_size))\n",
    "#         z = torch.from_numpy(z).float().to(device)\n",
    "#         fake_images = G(z, ages)\n",
    "#         D_fake = D(fake_images, ages)\n",
    "#         g_loss = real_loss(D_fake)\n",
    "#         test_loss += g_loss.item() * batch_size\n",
    "        \n",
    "#         # Calculate the predicted ages and the MAE\n",
    "#         predicted_ages = unnormalize_age(predict_age(fake_images, bins))\n",
    "#         true_ages = batch['age']\n",
    "#         mae += torch.sum(torch.abs(predicted_ages - true_ages)).item()\n",
    "#         num_batches += 1\n",
    "\n",
    "# # Calculate the average test loss and MAE\n",
    "# test_loss /= len(test_loader.dataset)\n",
    "# mae /= len(test_loader.dataset)\n",
    "\n",
    "# # Print the results\n",
    "# print('Test Loss: {:.4f}, MAE: {:.2f}'.format(test_loss, mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1938219",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
